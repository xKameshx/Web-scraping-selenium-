{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d80418cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Website: Pryka\n",
      "Category: Dress & Maxis\n",
      "Subcategory: \n",
      "Bali\n",
      "Clothes\n",
      "Collections\n",
      "Dress & Maxis\n",
      "Sizes: L, M, S, XL, XS, XXL\n",
      "Product Title: Margarita by the Beach Maxi Dress\n",
      "Image 1/6 saved: 1-6.jpg\n",
      "Image 2/6 saved: 2-6.jpg\n",
      "Image 3/6 saved: 3-6.jpg\n",
      "Image 4/6 saved: 4-6.jpg\n",
      "Image 5/6 saved: 5-6.jpg\n",
      "Image 6/6 saved: 6-6.jpg\n",
      "All 6 images saved to D:\\VS Code Projects\\Pryka Intern\\Pryka\\Margarita by the Beach Maxi Dress\\Product Images\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "import urllib\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "def save_to_json(data, website, category):\n",
    "    # Create a folder for the website if it doesn't exist\n",
    "    website_dir = os.path.join(os.getcwd(), website)\n",
    "    os.makedirs(website_dir, exist_ok=True)\n",
    "\n",
    "    # Create a folder for the category under the website directory\n",
    "    category_dir = os.path.join(website_dir, category)\n",
    "    os.makedirs(category_dir, exist_ok=True)\n",
    "\n",
    "    # Define the filename (e.g., \"Category.json\")\n",
    "    filename = os.path.join(category_dir, f\"{category}.json\")\n",
    "\n",
    "    # Save data to the JSON file\n",
    "    with open(filename, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "\n",
    "def scrape_zara_product(url):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    website = 'Zara'\n",
    "\n",
    "    # Extract the title of the webpage\n",
    "    webpage_title = driver.title\n",
    "\n",
    "    # Split the title using the dash character\n",
    "    title_parts = webpage_title.split('-')\n",
    "\n",
    "    # Check if there are enough elements in title_parts\n",
    "    if len(title_parts) >= 2:\n",
    "        # Extract the category and color\n",
    "        category_parts = title_parts[-2].strip().split()\n",
    "\n",
    "        # Extract the last word from the category\n",
    "        category = category_parts[-1]\n",
    "    else:\n",
    "        category = \"Category Not Found\"\n",
    "\n",
    "    # Extract the color and additional info using the pipe character\n",
    "    color_parts = title_parts[-1].strip().split('|')\n",
    "\n",
    "    # Extract the color\n",
    "    color = color_parts[0].strip()\n",
    "\n",
    "    try:\n",
    "        # Wait for the sizes element to be present for a maximum of 10 seconds\n",
    "        sizes_element = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, 'size-selector-list__wrapper--grid-gap'))\n",
    "        )\n",
    "\n",
    "        # Locate the sizes\n",
    "        size_items = sizes_element.find_elements(By.XPATH, './/li[@role=\"option\"]')\n",
    "\n",
    "        # Extract and print the sizes excluding \"VIEW SIMILAR\"\n",
    "        sizes = [size_item.text for size_item in size_items if \"VIEW SIMILAR\" not in size_item.text]\n",
    "        sizes_text = ', '.join(sizes)\n",
    "        print(\"Sizes: \" + sizes_text)\n",
    "\n",
    "    except TimeoutException:\n",
    "        # Handle the case where sizes are not present\n",
    "        print(\"Sizes are not available for this product.\")\n",
    "\n",
    "    # Print the category and color\n",
    "    print(\"Category: \" + category)\n",
    "    print(\"Color: \" + color)\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # Extract and print the text content of the <h1> element with the specified class and attribute\n",
    "    h1_element = soup.find('h1', class_='product-detail-info__header-name', attrs={'data-qa-qualifier': 'product-detail-info-name'})\n",
    "    if h1_element:\n",
    "        product_title = h1_element.get_text(strip=True)\n",
    "        print(\"Product Title:\", product_title)\n",
    "    else:\n",
    "        print(\"Product Title Not Found\")\n",
    "    \n",
    "    # Save the print text data into a JSON file\n",
    "    data = {\n",
    "        \"Sizes\": sizes_text,\n",
    "        \"Category\": category,\n",
    "        \"Color\": color,\n",
    "    }\n",
    "\n",
    "    save_to_json(data, website, product_title)\n",
    "\n",
    "\n",
    "    # Close the browser window\n",
    "    driver.quit()\n",
    "    \n",
    "def scrape_pryka_product_with_images(url):\n",
    "    def scrape_and_save_images(driver, website_name, category):\n",
    "        # Extract the HTML content\n",
    "        page_source = driver.page_source\n",
    "\n",
    "        # Parse the HTML with BeautifulSoup\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        # Find the image URLs in the specified ol element\n",
    "        image_ol = soup.find('ol', class_='flex-control-nav flex-control-thumbs')\n",
    "        if image_ol:\n",
    "            image_urls = [img['src'] for img in image_ol.find_all('img', src=True)]\n",
    "            num_images = len(image_urls)\n",
    "\n",
    "            # Create a directory for the website if it doesn't exist\n",
    "            website_dir = os.path.join(os.getcwd(), website_name)\n",
    "            os.makedirs(website_dir, exist_ok=True)\n",
    "\n",
    "            # Create a directory for the category if it doesn't exist\n",
    "            category_dir = os.path.join(website_dir, category)\n",
    "            os.makedirs(category_dir, exist_ok=True)\n",
    "\n",
    "            # Create a directory for the product if it doesn't exist\n",
    "            product_dir = os.path.join(category_dir, f\"Product Images\")\n",
    "            os.makedirs(product_dir, exist_ok=True)\n",
    "\n",
    "            # Download and save each image\n",
    "            for i, image_url in enumerate(image_urls, start=1):\n",
    "                response = requests.get(image_url)\n",
    "                if response.status_code == 200:\n",
    "                    image_extension = image_url.split('.')[-1]\n",
    "                    filename = f\"{i}-{num_images}.{image_extension}\"\n",
    "                    filepath = os.path.join(product_dir, filename)\n",
    "\n",
    "                    with open(filepath, 'wb') as f:\n",
    "                        f.write(response.content)\n",
    "\n",
    "                    print(f\"Image {i}/{num_images} saved: {filename}\")\n",
    "                else:\n",
    "                    print(f\"Failed to download image {i}: {image_url}\")\n",
    "\n",
    "            print(f\"All {num_images} images saved to {product_dir}\")\n",
    "\n",
    "        else:\n",
    "            print(\"No images found on the page.\")\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Extract the website name from the URL\n",
    "    website_name = \"Pryka\"\n",
    "\n",
    "    # Wait for the span with class \"posted_in\" to be present\n",
    "    categories_element = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, 'posted_in'))\n",
    "    )\n",
    "\n",
    "    # Extract the categories\n",
    "    categories = categories_element.find_elements(By.XPATH, './/a[@rel=\"tag\"]')\n",
    "    print(f\"Website: {website_name}\")\n",
    "    \n",
    "    # Extract and print the last category\n",
    "    if categories:\n",
    "        last_category = categories[-1].text\n",
    "        print(\"Category: \" + last_category)\n",
    "    else:\n",
    "        print(\"Category Not Found\")\n",
    "        \n",
    "    if categories:\n",
    "        print(\"Subcategory: \")\n",
    "        for category in categories:\n",
    "            print(category.text)\n",
    "    else:\n",
    "        print(\"Categories Not Found\")\n",
    "        \n",
    "    # Extract the colors\n",
    "    colors_element = driver.find_elements(By.XPATH, '//span[@data-sheets-value]')\n",
    "    colors = []\n",
    "\n",
    "    if colors_element:\n",
    "        # Skip the first element and find all non-empty colors\n",
    "        non_empty_colors = [color.text.strip() for color in colors_element[1:] if color.text.strip()]\n",
    "        colors = non_empty_colors\n",
    "\n",
    "    if colors:\n",
    "        print(\"Colors:\")\n",
    "        for color in colors:\n",
    "            print(color)\n",
    "    \n",
    "    # Extract and print the fabric\n",
    "    fabric_element = driver.find_element(By.XPATH, '//span[@style=\"font-weight: 400;\"]/span[@data-sheets-value]')\n",
    "    fabric = fabric_element.text.strip()\n",
    "    #print(\"Fabric:\", fabric)\n",
    "    \n",
    "\n",
    "    # Click on the \"Additional Information\" tab\n",
    "    additional_info_tab = driver.find_element(By.XPATH, '//a[@href=\"#tab-additional_information\"]')\n",
    "    additional_info_tab.click()\n",
    "\n",
    "    # Wait for the sizes element to be present for a maximum of 10 seconds\n",
    "    sizes_element = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, '//th[text()=\"Size\"]/following-sibling::td/p'))\n",
    "    )\n",
    "\n",
    "    # Extract and print the sizes\n",
    "    sizes_text = sizes_element.text.strip()\n",
    "    print(\"Sizes:\", sizes_text)\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # Extract and print the text content of the <h1> element with the specified class\n",
    "    h1_element = soup.find('h1', class_='product_title entry-title')\n",
    "    if h1_element:\n",
    "        product_title = h1_element.get_text(strip=True)\n",
    "        print(\"Product Title:\", product_title)\n",
    "    else:\n",
    "        print(\"Product Title Not Found\")\n",
    "    \n",
    "    scrape_and_save_images(driver, website_name, product_title)\n",
    "   \n",
    "    # Create a dictionary to store the information\n",
    "    data = {\n",
    "        \"Website\": website_name,\n",
    "        \"Category\": last_category,\n",
    "        \"Subcategories\": [category.text for category in categories] if categories else [],\n",
    "        \"Sizes\": sizes_text,\n",
    "    }\n",
    "    \n",
    "    save_to_json(data, website_name, product_title)\n",
    "\n",
    "    # Close the browser window\n",
    "    driver.quit()\n",
    "    \n",
    "def scrape_papa_product(url):\n",
    "    \n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    website = \"Papa Don't Preach\"\n",
    "\n",
    "    # Wait for the product title to be present\n",
    "    product_title_element = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, '//div[@class=\"product__title\"]/h1'))\n",
    "    )\n",
    "\n",
    "    # Extract the full product title\n",
    "    full_title = product_title_element.text.strip()\n",
    "\n",
    "    # Find the index of the '-' symbol\n",
    "    dash_index = full_title.find('-')\n",
    "\n",
    "    if dash_index != -1 and dash_index + 1 < len(full_title):\n",
    "        # Extract and print the text after the '-' symbol\n",
    "        category = full_title[dash_index + 1:].strip()\n",
    "        print(\"Category:\", category)\n",
    "    else:\n",
    "        print(\"Category Not Found\")\n",
    "\n",
    "    # Wait for the breadcrumbs to be present\n",
    "    breadcrumbs_element = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, '//div[@class=\"page-width breadcrumbs\"]'))\n",
    "    )\n",
    "\n",
    "    # Extract and print the breadcrumbs\n",
    "    breadcrumbs = breadcrumbs_element.find_elements(By.XPATH, './/a')\n",
    "\n",
    "    if breadcrumbs:\n",
    "        print(\"Subcategories:\")\n",
    "        for breadcrumb in breadcrumbs[1:]:\n",
    "            print(breadcrumb.text)\n",
    "    else:\n",
    "        print(\"Subcategories Not Found\")\n",
    "\n",
    "    # Click on the \"Product Information\" tab to activate it\n",
    "    product_info_tab_element = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, '//li[@class=\"accordion-item\"]/h3[text()=\"Product Information\"]'))\n",
    "    )\n",
    "    product_info_tab_element.click()\n",
    "\n",
    "    # Wait for the \"Product Information\" panel to be present\n",
    "    product_info_element = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, '//p[@class=\"accordion-panel\"]'))\n",
    "    )\n",
    "\n",
    "    # Wait for the product image to be present\n",
    "    product_image_element = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, '//div[@class=\"product__media media media--transparent\"]/img'))\n",
    "    )\n",
    "    \n",
    "    # Extract and print color and composition\n",
    "    color = re.search(r'Color:\\s*(\\w+)', product_info_element.text)\n",
    "    composition = re.search(r'Composition:\\s*([\\w\\s,]+)', product_info_element.text)\n",
    "\n",
    "    print(\"Color:\", color.group(1) if color else \"Not Found\")\n",
    "    print(\"Composition:\", composition.group(1) if composition else \"Not Found\")\n",
    "    \n",
    "    \n",
    "    # Extract color and composition\n",
    "    color_match = re.search(r'Color:\\s*(\\w+)', product_info_element.text)\n",
    "    composition_match = re.search(r'Composition:\\s*([\\w\\s,]+)', product_info_element.text)\n",
    "\n",
    "    color = color_match.group(1) if color_match else \"Not Found\"\n",
    "    composition = composition_match.group(1) if composition_match else \"Not Found\"\n",
    "    subcategories = [breadcrumb.text for breadcrumb in breadcrumbs[1:]] if breadcrumbs else []\n",
    "    color = color_match.group(1) if color_match else \"Not Found\"\n",
    "    composition = composition_match.group(1) if composition_match else \"Not Found\"\n",
    "    \n",
    "    # Create a dictionary to store the information\n",
    "    data = {\n",
    "        \"Website\": website,\n",
    "        \"Category\": category,\n",
    "        \"Subcategories\": subcategories,\n",
    "        \"Color\": color,\n",
    "        \"Composition\": composition\n",
    "    }\n",
    "    \n",
    "    save_to_json(data, website, category)\n",
    "\n",
    "    \n",
    "    # Close the browser window\n",
    "    driver.quit()\n",
    "    \n",
    "    \n",
    "def scrape_kshitijjalori_product(url):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    website = \"Kshitij Jalori\"\n",
    "\n",
    "    # Wait for the product title to be present\n",
    "    product_title_element = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, '//h1[@class=\"h2 product-single__title\"]'))\n",
    "    )\n",
    "\n",
    "    # Extract the product title\n",
    "    product_title = product_title_element.text.strip()\n",
    "\n",
    "    # Remove numbers and \"/\"\n",
    "    product_title_cleaned = re.sub(r'^\\d+/\\d+\\s+', '', product_title)\n",
    "    print(\"Category:\", product_title_cleaned)\n",
    "\n",
    "    # Create folder structure\n",
    "    folder_structure = os.path.join(website, product_title_cleaned)\n",
    "    os.makedirs(folder_structure, exist_ok=True)\n",
    "\n",
    "    # Wait for the product description to be present\n",
    "    product_description_element = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, '//div[@class=\"product-single__description rte\"]'))\n",
    "    )\n",
    "\n",
    "    # Extract and print the product details excluding the first <p>\n",
    "    product_details_elements = product_description_element.find_elements(By.XPATH, './/p')[1:]\n",
    "    product_details = '\\n'.join([element.text.strip() for element in product_details_elements])\n",
    "    print(\"Product Details:\")\n",
    "    print(product_details)\n",
    "\n",
    "    # Extract image URL\n",
    "    image_element = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, '//div[@class=\"image-wrap\"]/img[contains(@class, \"photoswipe__image\")]'))\n",
    "    )\n",
    "    image_relative_url = image_element.get_attribute('data-photoswipe-src')\n",
    "    image_absolute_url = f'http:{image_relative_url}'\n",
    "\n",
    "    # Download and save the image\n",
    "    image_filename = f\"{product_title_cleaned}.jpg\"\n",
    "    image_filepath = os.path.join(folder_structure, image_filename)\n",
    "    urllib.request.urlretrieve(image_absolute_url, image_filepath)\n",
    "    print(f\"Image downloaded and saved: {image_filepath}\")\n",
    "    \n",
    "    data = {\n",
    "        \"Website\": website,\n",
    "        \"Category\": product_title_cleaned,\n",
    "        \"ProductDetails\": product_details\n",
    "    }\n",
    "    \n",
    "    save_to_json(data, website, product_title_cleaned)\n",
    "\n",
    "    # Close the browser window\n",
    "    driver.quit()\n",
    "\n",
    "    \n",
    "def scrape_gucci_website(url):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    website = \"Gucci\"\n",
    "    \n",
    "    # Wait for the title meta tag to be present\n",
    "    title_meta_element = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, 'meta[name=\"title\"]'))\n",
    "    )\n",
    "\n",
    "    # Extract the content of the title meta tag\n",
    "    category_full = title_meta_element.get_attribute(\"content\")\n",
    "\n",
    "    # Split the category by '|', take the first part, and remove leading/trailing whitespaces\n",
    "    category_unsplit = category_full.split('|')[0].strip()\n",
    "    \n",
    "    parts = category_unsplit.split('in')\n",
    "    \n",
    "    category = parts[0].strip()\n",
    "    color = parts[1].strip() if len(parts) > 1 else ''  # Handle the case where 'in' is not present\n",
    "\n",
    "    print(\"Category:\", category)\n",
    "    print(\"Color:\", color)\n",
    "    \n",
    "    # Find the accordion drawer and scroll into view\n",
    "    accordion_drawer = driver.find_element(By.ID, 'product-details')\n",
    "    driver.execute_script(\"arguments[0].scrollIntoView();\", accordion_drawer)\n",
    "\n",
    "    # Wait for the details to be present\n",
    "    details_element = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, 'product-detail'))\n",
    "    )\n",
    "\n",
    "    # Extract and print the details using BeautifulSoup\n",
    "    soup = BeautifulSoup(details_element.get_attribute(\"outerHTML\"), 'html.parser')\n",
    "    details_items = soup.select('.product-detail li')\n",
    "    \n",
    "    # Define keywords to exclude\n",
    "    exclude_keywords = ['made in', 'the model is', 'shoulder', 'back length', 'guarantees', 'image']\n",
    "\n",
    "    # Filter out lines containing exclude_keywords\n",
    "    details_text = '\\n'.join(item.get_text(strip=True) for item in details_items if not any(keyword in item.get_text(strip=True).lower() for keyword in exclude_keywords))\n",
    "\n",
    "    print(\"Details:\")\n",
    "    print(details_text)\n",
    "    \n",
    "    data = {\n",
    "        \"Website\": website,\n",
    "        \"Category\": category,\n",
    "        \"Color\": color,\n",
    "        \"ProductDetails\": details_text\n",
    "    }\n",
    "    \n",
    "    save_to_json(data, website, category)\n",
    "    \n",
    "    # Close the browser window\n",
    "    driver.quit()\n",
    "    \n",
    "    \n",
    "# Example usage:\n",
    "url = 'https://pryka.in/product/off-white-bloom-maxi-dress/'\n",
    "if 'zara.com' in url:\n",
    "    scrape_zara_product(url)\n",
    "elif 'pryka.in' in url:\n",
    "    scrape_pryka_product_with_images(url)\n",
    "elif 'papadontpreach.com' in url:\n",
    "    scrape_papa_product(url)\n",
    "elif 'kshitijjalori.com' in url:\n",
    "    scrape_kshitijjalori_product(url)\n",
    "elif 'gucci.com' in url:\n",
    "    scrape_gucci_website(url)\n",
    "else:\n",
    "    print(\"Unsupported website.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eac8025",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
